{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MleqVXF_-kV7"
      },
      "outputs": [],
      "source": [
        "# Installs Kaggle API; allows us to easily use Kaggle datasets\n",
        "# Want to go to Kaggle > Settings > API > Create New Token (and then download kaggle.json file)\n",
        "!pip install kaggle\n",
        "\n",
        "# Allows us to access local files\n",
        "from google.colab import files\n",
        "\n",
        "# Prompts you to upload files; want to upload kaggle.json\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Creates a new directory and copies kaggle.json file to directory; specify read/write permissions only to owner\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Lists Kaggle datasets\n",
        "!kaggle datasets list\n",
        "\n",
        "# Downloads specified dataset; can obtain this command by copying the API command of a dataset. This can be found by clicking the three dots on a given dataset near the download button.\n",
        "!kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d-jzHGxs_gLY"
      },
      "outputs": [],
      "source": [
        "# Allows us to work with zip files since Kaggle will give our dataset in .zip format\n",
        "import zipfile\n",
        "\n",
        "# Specifies location of zip file\n",
        "data_zip = \"/content/brain-mri-images-for-brain-tumor-detection.zip\"\n",
        "\n",
        "# Specifies location to put the unzipped contents in\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Reading zip file and extracting its contents into data_dir ('./data')\n",
        "data_zip_ref = zipfile.ZipFile(data_zip,\"r\")\n",
        "data_zip_ref.extractall(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZqyqdMmBeun",
        "outputId": "b96e8198-b621-4528-c670-60cfdcf24d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 253 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Loads dataset from given directory (uses default specifications such as batch size 32, reshapes images to 256 x 256, and shuffling data)\n",
        "# Can specify alternate values depending on computational capabilities (i.e. data = tf.keras.utils.image_dataset_from_directory('/content/data/chest_xray/train', batch_size = 16))\n",
        "# If limited by computational capabilities, want to reduce batch size\n",
        "data = tf.keras.utils.image_dataset_from_directory('/content/data/brain_tumor_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vOYE1a3pBrPn"
      },
      "outputs": [],
      "source": [
        "# This code block is just to dive deeper into what 'data' contains and how we work with it\n",
        "\n",
        "# Allows us to access batches of our dataset from our data pipeline\n",
        "iter = data.as_numpy_iterator()\n",
        "\n",
        "# Has the first batch of training data; batch[0] will have image data (each image is a 2-d array with color channels of each pixel), batch[1] will have labels\n",
        "batch = iter.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pIFRTB6eDLeL"
      },
      "outputs": [],
      "source": [
        "# Currently, image color channels have values ranging from 0 to 255; we want to scale our images and have their color channel values between 0 and 1.\n",
        "# Can transform data as it is loaded in the data pipeline using map function (a kind of lamba function)\n",
        "# x is an image, y is the label associated with the image\n",
        "\n",
        "data = data.map(lambda x, y: (x/255, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJzKp78VFPLm",
        "outputId": "9c2aad18-79bf-4bee-df4e-464137ebe0e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We want to split our batches into training, validation, and testing batches\n",
        "# Can do this knowing the number of batches\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yAXkN74mFXFn"
      },
      "outputs": [],
      "source": [
        "# The number of allocated training, validation, and testing batches should be equal to the total number of batches\n",
        "\n",
        "train_size = int(len(data) * .8)\n",
        "validation_size = int(len(data) * .1) + 1\n",
        "test_size = int(len(data) * .1) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5QSlBh6FbNZ",
        "outputId": "c1d2230f-e3c2-491a-c6ca-9504a9123c13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Since len(data) = train_size + validation_size + test_size, we can proceed\n",
        "\n",
        "train_size + validation_size + test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iHXfRevbFi8b"
      },
      "outputs": [],
      "source": [
        "# Training data trains the model\n",
        "# Validation data is checks our model's performance during training, and it is used for fine tuning\n",
        "# Testing data is for evaluating the model at the end\n",
        "\n",
        "# There are data pipeline functions to select batches for training, validation, and testing partitions\n",
        "\n",
        "# Takes the allocated number of batches for training (first [train_size] batches)\n",
        "train = data.take(train_size)\n",
        "\n",
        "# Skips first [train_size batches] and takes the number of batches allocated to validation from the leftover batches\n",
        "validation = data.skip(train_size).take(validation_size)\n",
        "\n",
        "# Skips the batches allocated for training and validation, taking rest for testing\n",
        "test = data.skip(train_size + validation_size).take(test_size)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
