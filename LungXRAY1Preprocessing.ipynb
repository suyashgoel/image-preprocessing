{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg279tiPhWCJ"
      },
      "outputs": [],
      "source": [
        "# Installs Kaggle API; allows us to easily use Kaggle datasets\n",
        "# Want to go to Kaggle > Settings > API > Create New Token (and then download kaggle.json file)\n",
        "!pip install kaggle\n",
        "\n",
        "# Allows us to access local files\n",
        "from google.colab import files\n",
        "\n",
        "# Prompts you to upload files; want to upload kaggle.json\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Creates a new directory and copies kaggle.json file to directory; specify read/write permissions only to owner\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Lists Kaggle datasets\n",
        "!kaggle datasets list\n",
        "\n",
        "# Downloads specified dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Allows us to work with zip files since Kaggle will give our dataset in .zip format\n",
        "import zipfile\n",
        "\n",
        "# Specifies location of zip file\n",
        "data_zip = \"/content/chest-xray-pneumonia.zip\"\n",
        "\n",
        "# Specifies location to put unzipped contents in\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Reading zip file and extracting contents into data_dir ('./data')\n",
        "data_zip_ref = zipfile.ZipFile(data_zip,\"r\")\n",
        "data_zip_ref.extractall(data_dir)"
      ],
      "metadata": {
        "id": "ZwuadWeZn3Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use tensorflow to train our neural network\n",
        "import tensorflow as tf\n",
        "\n",
        "#ImageDataGenerator allows us to augment our data\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "-d-UpS_Uv4gK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variables that store the paths to our training, validation, and testing data\n",
        "train_dir = '/content/data/chest_xray/train'\n",
        "val_dir = '/content/data/chest_xray/val'\n",
        "test_dir = '/content/data/chest_xray/test'\n",
        "\n",
        "# This specifies the transformations and augmentations to be done to the specified data\n",
        "# Rescale normalizes by multiplying color values by 1/255\n",
        "# Rotation range randomly rotates images during training by up to 30 degrees\n",
        "# Width shift range randomly shifts images left or right up to 20% of width\n",
        "# Height shift range randomly shifts images left or right up to 20% of height\n",
        "# Zoom range randomly zooms images in or out up to 20% of image size\n",
        "# Brightness range randomly varies brightness from 25% to 100% of actual image brightness\n",
        "# Augmentation helps us have more diverse data, which can be useful since real-world input can vary\n",
        "datagen = ImageDataGenerator(rescale = 1.0/255, rotation_range = 30, width_shift_range = 0.2, height_shift_range = 0.2, zoom_range=0.2, brightness_range=(0.25, 1))\n",
        "\n",
        "# Partitions our data into training, testing, and validation and applies specified transformations. Specifies that this is a binary classification problem. Default batch (set of samples of a given size that make up one iteration of training) sizes of 32 and image sizes of 256 x 256\n",
        "# We can vary batch size based on computational capabilities. Lower batch size is less expensive\n",
        "# Training trains our model, validation helps us fine tune the model by evaluating the model during training, and testing is for after we are done training\n",
        "train_generator = datagen.flow_from_directory(train_dir, class_mode='binary')\n",
        "test_generator = datagen.flow_from_directory(test_dir, class_mode='binary')\n",
        "val_generator = datagen.flow_from_directory(val_dir, class_mode='binary')\n",
        "\n",
        "# Since we have resized, normalized, and augmented our data, we have adequately preprocessed the data and can proceed with training a model.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGSgKo_I0-IG",
        "outputId": "b7741f8b-b3dc-4924-ef70-690c46c7b565"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    }
  ]
}